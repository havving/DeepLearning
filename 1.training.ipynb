{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random, os, time\n",
    "import tensorflow.contrib.slim as slim  # 기존 tensorflow를 사용하기 쉽게 만들어 놓은 high-level API\n",
    "\n",
    "from custom_op import conv2d, conv2d_t, atrous_conv2d, relu, bn, max_pool\n",
    "from utils import read_data_path, next_batch, read_image, read_annotation, draw_plot_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLab_v3(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        RESULT_MODEL_PATH = '/src/hyebin/modellos/DEEPLAB-V3-CRACK-11/'\n",
    "\n",
    "        # 최대 학습 수행 횟수\n",
    "        self.N_EPOCH = 20\n",
    "        # 총 이미지 개수에서 나눌 학습 수행 횟수l\n",
    "        self.N_BATCH = 5\n",
    "        # 최적화 함수 학습률\n",
    "        self.LEARNING_RATE = 0.00001\n",
    "        \n",
    "        self.LOGS_DIR = os.path.join(RESULT_MODEL_PATH ,'logs')\n",
    "        self.CKPT_DIR = os.path.join(RESULT_MODEL_PATH ,'ckpt')\n",
    "        self.OUTPUT_DIR = os.path.join(RESULT_MODEL_PATH ,'output')\n",
    "        \n",
    "        self.N_CLASS = 151  # filters\n",
    "        self.RESIZE = 191\n",
    "        \n",
    "        # '/src/data/DeepCrack/train/train_img/'\n",
    "        # '/src/hyebin/deep-crack/ADEChallengeData2016/images/training/'\n",
    "        self.TRAIN_IMAGE_PATH = '/src/data/DeepCrack/train/train_img/'\n",
    "        # '/src/data/DeepCrack/train/train_lab/'\n",
    "        # '/src/hyebin/deep-crack/ADEChallengeData2016/annotations/training/'\n",
    "        self.TRAIN_LABEL_PATH = '/src/data/DeepCrack/train/train_lab/'\n",
    "    \n",
    "        # '/src/data/DeepCrack/train/test_img/'\n",
    "        # '/src/hyebin/deep-crack/ADEChallengeData2016/images/validation/'\n",
    "        self.VALID_IMAGE_PATH = '/src/data/DeepCrack/train/test_img/'\n",
    "        # '/src/data/DeepCrack/train/test_lab/'\n",
    "        # '/src/hyebin/deep-crack/ADEChallengeData2016/annotations/validation/'\n",
    "        self.VALID_LABEL_PATH = '/src/data/DeepCrack/train/tlest_lab/'\n",
    "        \n",
    "        \n",
    "        \n",
    "    ## 모델 생성\n",
    "    def make_model(self, inputs, is_training):\n",
    "    \n",
    "        ## ResNet을 사용하여 특징 추출 (Encoder)\n",
    "    \n",
    "        # ResNet50 : Convolution 연산 + Fully Connected Layer만 계산했을 때, 총 50개의 Layer (CNN)\n",
    "        # layer가 깊어질수록 성능이 더 좋아진다고 생각 (VGG-19보다 더 깊게 설계)\n",
    "        with tf.variable_scope('ResNet50'):\n",
    "            # strides : 필터를 적용하는 간격 (필터의 이동량)\n",
    "            x = conv2d(inputs, 64, [7,7], strides=[1,2,2,1], name='conv1')   # size 1/2\n",
    "            # bn : tf.contrib.layers.batch_norm (Batch Normalization)\n",
    "            # Batch Normalization : 신경망을 안정시켜주는 표준화 기법 \n",
    "            x = bn(x, is_training)\n",
    "            # relu : tf.nn.relu\n",
    "            x = relu(x)\n",
    "            # max_pool : Pooling Layer (tf.nn.max_pool)\n",
    "            x = max_pool(x, ksize=[1,3,3,1], name='pool1')   # size 1/4\n",
    "            print('x : {}'.format(x))\n",
    "        \n",
    "            x = self.conv_block(x, [64, 64, 256], '2_1', is_training, s=1)\n",
    "            x = self.identity_block(x, [64, 64, 256], '2_2', is_training)\n",
    "            x = self.identity_block(x, [64, 64, 256], '2_3', is_training)\n",
    "            print('x1 : {}'.format(x))\n",
    "        \n",
    "            x = self.conv_block(x, [128, 128, 512], '3_1', is_training)\n",
    "            x = self.identity_block(x, [128, 128, 512], '3_2', is_training)\n",
    "            x = self.identity_block(x, [128, 128, 512], '3_3', is_training)\n",
    "            print('x2 : {}'.format(x))\n",
    "            \n",
    "            x = self.atrous_conv_block(x, [256, 256, 1024], '4_1', 2, is_training, s=1)\n",
    "            x = self.atrous_identity_block(x, [256, 256, 1024], '4_2',  2, is_training)\n",
    "            x = self.atrous_identity_block(x, [256, 256, 1024], '4_3',  2, is_training)\n",
    "            x = self.atrous_identity_block(x, [256, 256, 1024], '4_4',  2, is_training)\n",
    "            x = self.atrous_identity_block(x, [256, 256, 1024], '4_5',  2, is_training)\n",
    "            x = self.atrous_identity_block(x, [256, 256, 1024], '4_6',  2, is_training)\n",
    "            print('x3 : {}'.format(x))\n",
    "        \n",
    "            x = self.atrous_conv_block(x, [512, 512, 2048], '5_1', 4, is_training, s=1)\n",
    "            x = self.atrous_identity_block(x, [512, 512, 2048], '5_2', 4, is_training)\n",
    "            x = self.atrous_identity_block(x, [512, 512, 2048], '5_3', 4, is_training)\n",
    "            print('x4 : {}'.format(x))\n",
    "        \n",
    "        \n",
    "            ## Atrous Pyrimid Pooling (Decoder)\n",
    "        \n",
    "            # ASPP : Atrous + Spatial Pyramid Pooling\n",
    "            #      : rate를 다양하게 변환시켜 다양한 RF(Receptive Field)가 고려된 feature map을 생성할 수 있도록 함\n",
    "            with tf.variable_scope('ASPP'):\n",
    "                feature_map_shape = x.get_shape().as_list()\n",
    "            \n",
    "                # global average pooling\n",
    "                # feature맵의 width, height 평균을 냄\n",
    "                feature_map = tf.reduce_mean(x, [1,2], keepdims=True)  # keepdims:차원 유지 여부\n",
    "            \n",
    "                # conv2d : Convolution Layer\n",
    "                feature_map = conv2d(feature_map, 256, [1,1], name='gap_feature_map')\n",
    "                feature_map = tf.image.resize_bilinear(feature_map, [feature_map_shape[1], feature_map_shape[2]])\n",
    "                print('feature_map : {}'.format(feature_map))\n",
    "            \n",
    "                # rate : 필터의 픽셀 간의 거리를 나타내는 확장 비율\n",
    "                rate1 = conv2d(x, 256, [1,1], name='rate1')\n",
    "                print('rate1 : {}'.format(rate1))\n",
    "                rate6 = atrous_conv2d(x, 256, [3,3], rate=6, name='rate6')\n",
    "                print('rate6 : {}'.format(rate6))\n",
    "                rate12 = atrous_conv2d(x, 256, [3,3], rate=12, name='rate12')\n",
    "                print('rate12 : {}'.format(rate12))\n",
    "                rate18 = atrous_conv2d(x, 256, [3,3], rate=18, name='rate18')\n",
    "                print('rate18 : {}'.format(rate18))\n",
    "                concated = tf.concat([feature_map, rate1, rate6, rate12, rate18], axis=3)  # concat:tensor 객체를 횡(가로) 방향으로 연결 (axis=3차원)\n",
    "                print('concated : {}'.format(concated))\n",
    "            \n",
    "                net = conv2d(concated, 256, [1,1], name='net')\n",
    "                print('net : {}'.format(net))\n",
    "            \n",
    "                ## 모델 초기 설정\n",
    "                # logits : sigmoid 함수와 역함수 관계\n",
    "                logits = conv2d(net, self.N_CLASS, [1,1], name='logits')\n",
    "                # tf.image.resize(bilinear(입력 텐서, [높이, 가로]))\n",
    "                logits = tf.image.resize_bilinear(logits, size=[self.RESIZE, self.RESIZE], name='out')\n",
    "                print('logits : {}'.format(logits))\n",
    "            \n",
    "                # 예측값\n",
    "                pred = tf.argmax(logits, axis=3)  # argmax:1p차원 배열에서 가장 큰 값을 찾아 index return\n",
    "                pred = tf.expand_dims(pred, dim=3)  # expand_dims:axis로 지정된 차원을 추가\n",
    "                print('pred : {}'.format(pred))\n",
    "            \n",
    "                return logits, pred\n",
    "            \n",
    "            \n",
    "            \n",
    "    ## 학습 전 함수 정의\n",
    "    def build_model(self):\n",
    "    \n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, self.RESIZE, self.RESIZE, 3])\n",
    "        self.label_y = tf.placeholder(dtype=tf.int32, shape=[None, self.RESIZE, self.RESIZE, 1])\n",
    "        \n",
    "        self.is_train = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "        self.logits, self.pred = self.make_model(self.input_x, self.is_train)\n",
    "    \n",
    "        # loss 함수 정의\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=tf.squeeze(self.label_y, [3])))\n",
    "               \n",
    "        # loss 함수의 최적화 함수 정의\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).minimize(self.loss)\n",
    "    \n",
    "        self.loss_summary = tf.summary.merge([tf.summary.scalar('loss', self.loss)])\n",
    "    \n",
    "        model_vars = tf.trainable_variables()\n",
    "        slim.model_analyzer.analyze_vars(model_vars, print_info=True)    \n",
    "        \n",
    "      \n",
    "    \n",
    "    ## 학습 수행 및 모델 저장\n",
    "    def train_model(self):\n",
    "    \n",
    "        if not os.path.exists(self.LOGS_DIR):    os.mkdir(self.LOGS_DIR)\n",
    "        if not os.path.exists(self.CKPT_DIR):    os.mkdir(self.CKPT_DIR)\n",
    "        if not os.path.exists(self.OUTPUT_DIR):  os.mkdir(self.OUTPUT_DIR)\n",
    "       \n",
    "        train_set_path = read_data_path(self.TRAIN_IMAGE_PATH, self.TRAIN_LABEL_PATH)\n",
    "        valid_set_path = read_data_path(self.VALID_IMAGE_PATH, self.VALID_LABEL_PATH)\n",
    "    \n",
    "        ckpt_save_path = os.path.join(self.CKPT_DIR, 'DEEPLAB-V3_' + str(self.N_BATCH) + '_' + str(self.LEARNING_RATE))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                total_batch = int(len(train_set_path) / self.N_BATCH)\n",
    "                counter = 0\n",
    "\n",
    "                self.saver = tf.train.Saver()\n",
    "                self.writer = tf.summary.FileWriter(self.LOGS_DIR, sess.graph)\n",
    "\n",
    "                for epoch in range(self.N_EPOCH):\n",
    "                    total_loss = 0\n",
    "                    random.shuffle(train_set_path)           # 매 epoch마다 데이터셋 shuffling\n",
    "                    random.shuffle(valid_set_path)\n",
    "\n",
    "                    for i in range(total_batch):\n",
    "                        batch_xs_path, batch_ys_path = next_batch(train_set_path, self.N_BATCH, i)\n",
    "\n",
    "                        batch_xs = read_image(batch_xs_path, [self.RESIZE, self.RESIZE])\n",
    "                        batch_ys = read_annotation(batch_ys_path, [self.RESIZE, self.RESIZE])\n",
    "                        \n",
    "                        feed_dict = {self.input_x: batch_xs, self.label_y: batch_ys, self.is_train: True}\n",
    "                        \n",
    "                        # 학습 수행\n",
    "                        _, _loss_summary, _loss = sess.run([self.optimizer, self.loss_summary, self.loss], feed_dict=feed_dict)\n",
    "                        \n",
    "                        #print('loss : {}'.format(_loss))\n",
    "                        \n",
    "                        self.writer.add_summary(_loss_summary, counter)\n",
    "                        counter += 1\n",
    "                        total_loss += _loss\n",
    "\n",
    "                    ## validation 과정\n",
    "                    valid_xs_path, valid_ys_path = next_batch(valid_set_path, 4, 0)\n",
    "                    # 실제 이미지\n",
    "                    valid_xs = read_image(valid_xs_path, [self.RESIZE, self.RESIZE])                   \n",
    "                    # 라벨 이미지\n",
    "                    valid_ys = read_annotation(valid_ys_path, [self.RESIZE, self.RESIZE])\n",
    "                    \n",
    "                    # 예측 이미지?\n",
    "                    valid_pred = sess.run(self.pred, feed_dict={self.input_x: valid_xs, self.label_y: valid_ys, self.is_train:False})                    \n",
    "                    valid_pred = np.squeeze(valid_pred, axis=3)\n",
    "                    \n",
    "                    valid_ys = np.squeeze(valid_ys, axis=3)  # squeeze:차원 중 size가 1인 것을 찾아 Scala값으로 바꿔 해당 차원을 제거\n",
    "\n",
    "                    ## plotting and save figure\n",
    "                    img_save_path = self.OUTPUT_DIR + '/' + str(epoch).zfill(3) + '.png'\n",
    "                    draw_plot_segmentation(img_save_path, valid_xs, valid_pred, valid_ys)\n",
    "                    \n",
    "                    #print('total_loss : {}'.format(total_loss))\n",
    "                    \n",
    "                    print('Epoch:', '%03d' % (epoch + 1), 'Avg Loss: {:.6}\\t'.format(total_loss / total_batch))\n",
    "                    self.saver.save(sess, ckpt_save_path + '_' + str(epoch) + '.model', global_step=counter)\n",
    "            \n",
    "                self.saver.save(sess, ckpt_save_path + '_' + str(epoch) + '.model', global_step=counter)\n",
    "                print('Finish save model')\n",
    "                \n",
    "       \n",
    "    \n",
    "    ## 모든 convoltion 연산에 3x3 이하 크기의 커널 사용 \n",
    "    ## feature map의 크기가 같은 layer는 출력 feature map 갯수가 동일함\n",
    "    ## ResNet50 부터는 연산량을 줄이기 위해 Residual Block 내에 1x1, 3x3, 1x1 Convolution 연산을 쌓음\n",
    "    ## 1x1 Convolution 연산으로 feature map의 갯수를 줄였다가, 3x3 연산을 거친 후,\n",
    "    ## 1x1 Convolution 연산으로 차원을 늘려줌  (=병목 레이어)\n",
    "    \n",
    "    ## 차원이 같은 Residual Block\n",
    "    ## 입력값과 출력값의 크기를 맞춰줌\n",
    "    ## input, output 차원이 같을 때 사용되는 shortcut\n",
    "    def identity_block(self, inputs, filters, stage, is_training):\n",
    "        filter1, filter2, filter3 = filters\n",
    "        layer1 = relu(bn(conv2d(inputs, filter1, [1,1], name=stage+'_a_identity', padding='VALID'), is_training))\n",
    "        layer2 = relu(bn(conv2d(layer1, filter2, [3,3], name=stage+'_b_identity'), is_training))\n",
    "        layer3 = bn(conv2d(layer2, filter3, [1,1], name=stage+'_c_identity', padding='VALID'), is_training)\n",
    "        layer4 = relu(tf.add(layer3, inputs))\n",
    "        return layer4\n",
    "    \n",
    "    \n",
    "    ## Projection을 활용한 Residual Block\n",
    "    ## 차원이 다르기 때문에 Conv_layer를 통해 projection shortcut\n",
    "    ## projection shortcut : 입력값을 바로 더하지 않고 \n",
    "    ##           1x1 convolution 연산을 stride를 설정하여 freature map의 크기와 개수를 맞춰준 후 더해줌\n",
    "    def conv_block(self, inputs, depths, stage, is_training, s=2):\n",
    "        depth1, depth2, depth3 = depths\n",
    "        layer1 = relu(bn(conv2d(inputs, depth1, [1,1], name=stage+'_a_conv', strides=[1, s, s, 1], padding='VALID'), is_training))\n",
    "        layer2 = relu(bn(conv2d(layer1, depth2, [3,3], name=stage+'_b_conv'), is_training))\n",
    "        layer3 = bn(conv2d(layer2, depth3, [1,1], name=stage+'_c_conv', padding='VALID'), is_training)\n",
    "        shortcut = bn(conv2d(inputs, depth3, [1,1], name=stage+'_shortcut', strides=[1, s, s, 1], padding='VALID'), is_training)\n",
    "        layer4 = relu(tf.add(layer3, shortcut))\n",
    "        return layer4\n",
    "    \n",
    "    \n",
    "     \n",
    "    def atrous_identity_block(self, inputs, depths, stage, rate, is_training):\n",
    "        depth1, depth2, depth3 = depths\n",
    "        layer1 = relu(bn(atrous_conv2d(inputs, depth1, [1,1], rate, name=stage+'_a_identity'), is_training))\n",
    "        layer2 = relu(bn(atrous_conv2d(layer1, depth2, [3,3], rate, name=stage+'_b_identity'), is_training))\n",
    "        layer3 = bn(atrous_conv2d(layer2, depth3, [1,1], rate, name=stage+'_c_identity'), is_training)\n",
    "        layer4 = relu(tf.add(layer3, inputs))\n",
    "        return layer4\n",
    "    \n",
    "    \n",
    "    \n",
    "    def atrous_conv_block(self, inputs, depths, stage, rate, is_training, s=2):\n",
    "        depth1, depth2, depth3 = depths\n",
    "        layer1 = relu(bn(atrous_conv2d(inputs, depth1, [1,1], rate, name=stage+'_a_conv'), is_training))\n",
    "        layer2 = relu(bn(atrous_conv2d(layer1, depth2, [3,3], rate, name=stage+'_b_conv'), is_training))\n",
    "        layer3 = bn(atrous_conv2d(layer2, depth3, [1,1], rate, name=stage+'_c_conv'), is_training)\n",
    "        shortcut = bn(conv2d(inputs, depth3, [1,1], name=stage+'_shortcut', strides=[1, s, s, 1], padding='VALID'), is_training)\n",
    "        layer4 = relu(tf.add(layer3, shortcut))\n",
    "        return layer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /src/hyebin/deep-crack/custom_op.py:13: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /src/hyebin/deep-crack/custom_op.py:72: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "x : Tensor(\"ResNet50/pool1:0\", shape=(?, 48, 48, 64), dtype=float32)\n",
      "x1 : Tensor(\"ResNet50/Relu_9:0\", shape=(?, 48, 48, 256), dtype=float32)\n",
      "x2 : Tensor(\"ResNet50/Relu_18:0\", shape=(?, 24, 24, 512), dtype=float32)\n",
      "x3 : Tensor(\"ResNet50/Relu_36:0\", shape=(?, 24, 24, 1024), dtype=float32)\n",
      "x4 : Tensor(\"ResNet50/Relu_45:0\", shape=(?, 24, 24, 2048), dtype=float32)\n",
      "feature_map : Tensor(\"ResNet50/ASPP/ResizeBilinear:0\", shape=(?, 24, 24, 256), dtype=float32)\n",
      "rate1 : Tensor(\"ResNet50/ASPP/rate1/BiasAdd:0\", shape=(?, 24, 24, 256), dtype=float32)\n",
      "rate6 : Tensor(\"ResNet50/ASPP/rate6/BiasAdd:0\", shape=(?, 24, 24, 256), dtype=float32)\n",
      "rate12 : Tensor(\"ResNet50/ASPP/rate12/BiasAdd:0\", shape=(?, 24, 24, 256), dtype=float32)\n",
      "rate18 : Tensor(\"ResNet50/ASPP/rate18/BiasAdd:0\", shape=(?, 24, 24, 256), dtype=float32)\n",
      "concated : Tensor(\"ResNet50/ASPP/concat:0\", shape=(?, 24, 24, 1280), dtype=float32)\n",
      "net : Tensor(\"ResNet50/ASPP/net/BiasAdd:0\", shape=(?, 24, 24, 256), dtype=float32)\n",
      "logits : Tensor(\"ResNet50/ASPP/out:0\", shape=(?, 191, 191, 151), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "pred : Tensor(\"ResNet50/ASPP/ExpandDims:0\", shape=(?, 191, 191, 1), dtype=int64)\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "ResNet50/conv1/conv_weight:0 (float32_ref 7x7x3x64) [9408, bytes: 37632]\n",
      "ResNet50/conv1/conv_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/2_1_a_conv/conv_weight:0 (float32_ref 1x1x64x64) [4096, bytes: 16384]\n",
      "ResNet50/2_1_a_conv/conv_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/2_1_b_conv/conv_weight:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "ResNet50/2_1_b_conv/conv_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_2/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_2/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/2_1_c_conv/conv_weight:0 (float32_ref 1x1x64x256) [16384, bytes: 65536]\n",
      "ResNet50/2_1_c_conv/conv_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_3/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_3/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/2_1_shortcut/conv_weight:0 (float32_ref 1x1x64x256) [16384, bytes: 65536]\n",
      "ResNet50/2_1_shortcut/conv_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_4/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_4/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/2_2_a_identity/conv_weight:0 (float32_ref 1x1x256x64) [16384, bytes: 65536]\n",
      "ResNet50/2_2_a_identity/conv_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_5/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_5/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/2_2_b_identity/conv_weight:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "ResNet50/2_2_b_identity/conv_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_6/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_6/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/2_2_c_identity/conv_weight:0 (float32_ref 1x1x64x256) [16384, bytes: 65536]\n",
      "ResNet50/2_2_c_identity/conv_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_7/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_7/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/2_3_a_identity/conv_weight:0 (float32_ref 1x1x256x64) [16384, bytes: 65536]\n",
      "ResNet50/2_3_a_identity/conv_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_8/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_8/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/2_3_b_identity/conv_weight:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "ResNet50/2_3_b_identity/conv_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_9/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/BatchNorm_9/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "ResNet50/2_3_c_identity/conv_weight:0 (float32_ref 1x1x64x256) [16384, bytes: 65536]\n",
      "ResNet50/2_3_c_identity/conv_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_10/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_10/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/3_1_a_conv/conv_weight:0 (float32_ref 1x1x256x128) [32768, bytes: 131072]\n",
      "ResNet50/3_1_a_conv/conv_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_11/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_11/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/3_1_b_conv/conv_weight:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
      "ResNet50/3_1_b_conv/conv_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_12/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_12/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/3_1_c_conv/conv_weight:0 (float32_ref 1x1x128x512) [65536, bytes: 262144]\n",
      "ResNet50/3_1_c_conv/conv_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_13/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_13/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/3_1_shortcut/conv_weight:0 (float32_ref 1x1x256x512) [131072, bytes: 524288]\n",
      "ResNet50/3_1_shortcut/conv_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_14/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_14/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/3_2_a_identity/conv_weight:0 (float32_ref 1x1x512x128) [65536, bytes: 262144]\n",
      "ResNet50/3_2_a_identity/conv_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_15/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_15/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/3_2_b_identity/conv_weight:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
      "ResNet50/3_2_b_identity/conv_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_16/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_16/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/3_2_c_identity/conv_weight:0 (float32_ref 1x1x128x512) [65536, bytes: 262144]\n",
      "ResNet50/3_2_c_identity/conv_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_17/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_17/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/3_3_a_identity/conv_weight:0 (float32_ref 1x1x512x128) [65536, bytes: 262144]\n",
      "ResNet50/3_3_a_identity/conv_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_18/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_18/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/3_3_b_identity/conv_weight:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
      "ResNet50/3_3_b_identity/conv_bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_19/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/BatchNorm_19/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "ResNet50/3_3_c_identity/conv_weight:0 (float32_ref 1x1x128x512) [65536, bytes: 262144]\n",
      "ResNet50/3_3_c_identity/conv_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_20/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_20/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/4_1_a_conv/atrous_weight:0 (float32_ref 1x1x512x256) [131072, bytes: 524288]\n",
      "ResNet50/4_1_a_conv/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_21/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_21/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_1_b_conv/atrous_weight:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
      "ResNet50/4_1_b_conv/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_22/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_22/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_1_c_conv/atrous_weight:0 (float32_ref 1x1x256x1024) [262144, bytes: 1048576]\n",
      "ResNet50/4_1_c_conv/atrous_bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_23/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_23/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/4_1_shortcut/conv_weight:0 (float32_ref 1x1x512x1024) [524288, bytes: 2097152]\n",
      "ResNet50/4_1_shortcut/conv_bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_24/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_24/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/4_2_a_identity/atrous_weight:0 (float32_ref 1x1x1024x256) [262144, bytes: 1048576]\n",
      "ResNet50/4_2_a_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_25/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_25/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_2_b_identity/atrous_weight:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
      "ResNet50/4_2_b_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_26/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_26/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_2_c_identity/atrous_weight:0 (float32_ref 1x1x256x1024) [262144, bytes: 1048576]\n",
      "ResNet50/4_2_c_identity/atrous_bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_27/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_27/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/4_3_a_identity/atrous_weight:0 (float32_ref 1x1x1024x256) [262144, bytes: 1048576]\n",
      "ResNet50/4_3_a_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_28/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_28/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_3_b_identity/atrous_weight:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
      "ResNet50/4_3_b_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_29/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_29/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_3_c_identity/atrous_weight:0 (float32_ref 1x1x256x1024) [262144, bytes: 1048576]\n",
      "ResNet50/4_3_c_identity/atrous_bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_30/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_30/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/4_4_a_identity/atrous_weight:0 (float32_ref 1x1x1024x256) [262144, bytes: 1048576]\n",
      "ResNet50/4_4_a_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_31/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_31/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_4_b_identity/atrous_weight:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
      "ResNet50/4_4_b_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_32/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_32/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_4_c_identity/atrous_weight:0 (float32_ref 1x1x256x1024) [262144, bytes: 1048576]\n",
      "ResNet50/4_4_c_identity/atrous_bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_33/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_33/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/4_5_a_identity/atrous_weight:0 (float32_ref 1x1x1024x256) [262144, bytes: 1048576]\n",
      "ResNet50/4_5_a_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_34/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_34/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_5_b_identity/atrous_weight:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
      "ResNet50/4_5_b_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_35/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_35/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_5_c_identity/atrous_weight:0 (float32_ref 1x1x256x1024) [262144, bytes: 1048576]\n",
      "ResNet50/4_5_c_identity/atrous_bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_36/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_36/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/4_6_a_identity/atrous_weight:0 (float32_ref 1x1x1024x256) [262144, bytes: 1048576]\n",
      "ResNet50/4_6_a_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_37/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_37/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_6_b_identity/atrous_weight:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
      "ResNet50/4_6_b_identity/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_38/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/BatchNorm_38/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/4_6_c_identity/atrous_weight:0 (float32_ref 1x1x256x1024) [262144, bytes: 1048576]\n",
      "ResNet50/4_6_c_identity/atrous_bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_39/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/BatchNorm_39/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "ResNet50/5_1_a_conv/atrous_weight:0 (float32_ref 1x1x1024x512) [524288, bytes: 2097152]\n",
      "ResNet50/5_1_a_conv/atrous_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_40/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_40/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/5_1_b_conv/atrous_weight:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
      "ResNet50/5_1_b_conv/atrous_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_41/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_41/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/5_1_c_conv/atrous_weight:0 (float32_ref 1x1x512x2048) [1048576, bytes: 4194304]\n",
      "ResNet50/5_1_c_conv/atrous_bias:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_42/beta:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_42/gamma:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/5_1_shortcut/conv_weight:0 (float32_ref 1x1x1024x2048) [2097152, bytes: 8388608]\n",
      "ResNet50/5_1_shortcut/conv_bias:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_43/beta:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_43/gamma:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/5_2_a_identity/atrous_weight:0 (float32_ref 1x1x2048x512) [1048576, bytes: 4194304]\n",
      "ResNet50/5_2_a_identity/atrous_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_44/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_44/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/5_2_b_identity/atrous_weight:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
      "ResNet50/5_2_b_identity/atrous_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_45/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_45/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/5_2_c_identity/atrous_weight:0 (float32_ref 1x1x512x2048) [1048576, bytes: 4194304]\n",
      "ResNet50/5_2_c_identity/atrous_bias:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_46/beta:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_46/gamma:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/5_3_a_identity/atrous_weight:0 (float32_ref 1x1x2048x512) [1048576, bytes: 4194304]\n",
      "ResNet50/5_3_a_identity/atrous_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_47/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_47/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/5_3_b_identity/atrous_weight:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
      "ResNet50/5_3_b_identity/atrous_bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_48/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/BatchNorm_48/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "ResNet50/5_3_c_identity/atrous_weight:0 (float32_ref 1x1x512x2048) [1048576, bytes: 4194304]\n",
      "ResNet50/5_3_c_identity/atrous_bias:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_49/beta:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/BatchNorm_49/gamma:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "ResNet50/ASPP/gap_feature_map/conv_weight:0 (float32_ref 1x1x2048x256) [524288, bytes: 2097152]\n",
      "ResNet50/ASPP/gap_feature_map/conv_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/ASPP/rate1/conv_weight:0 (float32_ref 1x1x2048x256) [524288, bytes: 2097152]\n",
      "ResNet50/ASPP/rate1/conv_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/ASPP/rate6/atrous_weight:0 (float32_ref 3x3x2048x256) [4718592, bytes: 18874368]\n",
      "ResNet50/ASPP/rate6/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/ASPP/rate12/atrous_weight:0 (float32_ref 3x3x2048x256) [4718592, bytes: 18874368]\n",
      "ResNet50/ASPP/rate12/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/ASPP/rate18/atrous_weight:0 (float32_ref 3x3x2048x256) [4718592, bytes: 18874368]\n",
      "ResNet50/ASPP/rate18/atrous_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/ASPP/net/conv_weight:0 (float32_ref 1x1x1280x256) [327680, bytes: 1310720]\n",
      "ResNet50/ASPP/net/conv_bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "ResNet50/ASPP/logits/conv_weight:0 (float32_ref 1x1x256x151) [38656, bytes: 154624]\n",
      "ResNet50/ASPP/logits/conv_bias:0 (float32_ref 151) [151, bytes: 604]\n",
      "Total size of variables: 38826135\n",
      "Total bytes of variables: 155304540\n",
      "\n",
      "Start training after 5sec....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_path_len : 300\n",
      "labels_path_len : 300\n",
      "inputs_path_len : 237\n",
      "labels_path_len : 237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a891734b3708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nFinish training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7e06b97ac542>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                         \u001b[0;31m# 학습 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_loss_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                         \u001b[0;31m#print('loss : {}'.format(_loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model = DeepLab_v3()\n",
    "    \n",
    "    model.build_model()\n",
    "    print('\\nStart training after 5sec....\\n')\n",
    "    time.sleep(5)\n",
    "    \n",
    "    model.train_model()\n",
    "    print('\\nFinish training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
