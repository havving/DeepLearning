{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- config : utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 설정 변수 정의 ---\n",
    "# 작업 디렉토리 정의\n",
    "# 학습 데이터 파일명\n",
    "noise_list_file = \"/src/hyebin/esnoise/noise_spec_data.npy\"\n",
    "# 라벨 데이터 파일명\n",
    "noise_label_list_file = \"/src/hyebin/esnoise/noise_label_data.npy\"\n",
    "# 진동 분석 모델 디렉토리명\n",
    "modeldir = \"/src/hyebin/model/NOISE-EDGE-SPEC-01\"\n",
    "# tensorboard 용 log 디렉토리명\n",
    "tblogdir = \"/src/hyebin/logs\"\n",
    "\n",
    "# 학습 데이터 설정\n",
    "# 학습할 데이터의 개수\n",
    "training_count = 2000\n",
    "\n",
    "# 전결합층 설정\n",
    "# 전결합층 첫번째 레이어 unit 개수\n",
    "layer1_unit_count = 256\n",
    "# 전결합층 dropout 비율\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# 최적화 함수 설정\n",
    "# 최적화 함수 학습률\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# 학습 수행 설정\n",
    "# 최대 학습 수행 회수\n",
    "epoch_count = 5000\n",
    "# 학습 중단 시킬, 최소 loss값\n",
    "loss_limit = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load noise data: /src/hyebin/esnoise/noise_spec_data.npy\n",
      "load complete\n",
      "load noise label data: /src/hyebin/esnoise/noise_label_data.npy\n",
      "load complete\n"
     ]
    }
   ],
   "source": [
    "# --- 학습 데이터 로딩 ---\n",
    "# 데이터 구조\n",
    "# noise_list : 학습용 데이터(이미지 개수, 48, 64, 3)\n",
    "# noise_label_list : 학습용 라벨 데이터(이미지 개수, 18)\n",
    "print(\"load noise data:\", noise_list_file)\n",
    "noise_list = np.load(noise_list_file, allow_pickle=True)\n",
    "print(\"load complete\")\n",
    "\n",
    "print(\"load noise label data:\", noise_label_list_file)\n",
    "org_noise_label_list = np.load(noise_label_list_file, allow_pickle=True)\n",
    "\n",
    "# one hot encoding\n",
    "noise_label_list = []\n",
    "unit_matrix = np.identity(18)\n",
    "for index in org_noise_label_list:\n",
    "    one_hot = unit_matrix[index:index+1][0]\n",
    "    noise_label_list.append(one_hot)\n",
    "\n",
    "noise_label_list = np.array(noise_label_list)\n",
    "\n",
    "print(\"load complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1227 14:33:54.521958 140558034323264 deprecation.py:323] From <ipython-input-4-77bbbb81e3e6>:22: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W1227 14:33:54.527358 140558034323264 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1227 14:33:54.812944 140558034323264 deprecation.py:323] From <ipython-input-4-77bbbb81e3e6>:36: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W1227 14:33:54.954103 140558034323264 deprecation.py:323] From <ipython-input-4-77bbbb81e3e6>:65: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1227 14:33:55.196756 140558034323264 deprecation.py:323] From <ipython-input-4-77bbbb81e3e6>:66: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    }
   ],
   "source": [
    "# --- VGG Net 신경망 구성 ---\n",
    "#     입력층 : 48*64 의 Spectrogram 전처리 데이터\n",
    "#     특징 추출층 : 2개 합성곱 필터, 1개의 필터를 2회 수행 \n",
    "#     전 결합층 : 3개의 은닉층(층당 유닛 1024개) → 단층에서 발생하는 xor 문제 회피, \n",
    "#                1개의 출력층(유닛 5개)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# dropout 사용 여부\n",
    "dropout_rate = tf.placeholder_with_default(0, shape=[], name=\"dropout_rate\")\n",
    "\n",
    "# --- 입력층 ---\n",
    "# Spectrogram\n",
    "x = tf.placeholder(tf.float32, [None, 48, 64, 3], name=\"in\")\n",
    "\n",
    "# --- 특징 추출층 ---\n",
    "conv11 = tf.layers.conv2d(\n",
    "    inputs=x,\n",
    "    filters=32,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "conv12 = tf.layers.conv2d(\n",
    "    inputs=conv11,\n",
    "    filters=32,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "# 풀링으로 사이즈가 줄어듦\n",
    "# Spectrogram : 48 * 64 → 24 * 32\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "    inputs=conv12,\n",
    "    pool_size=[2, 2],\n",
    "    strides=2)\n",
    "\n",
    "conv21 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "conv22 = tf.layers.conv2d(\n",
    "    inputs=conv21,\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "\n",
    "# 풀링으로 사이즈가 줄어듦\n",
    "# Spectrogram : 24 * 32 → 12 * 16\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "    inputs=conv22,\n",
    "    pool_size=[2, 2],\n",
    "    strides=2)\n",
    "\n",
    "# --- 전 결합층 ---\n",
    "# 데이터 행렬 평탄화(flatten)\n",
    "# Spectrogram : 6 * 8(이미지사이즈) * 64(합성곱 필터 수)\n",
    "input_cnt = 12 * 16 * 64\n",
    "pool_flat = tf.reshape(pool2, [-1, input_cnt])\n",
    "\n",
    "dense1 = tf.layers.dense(inputs=pool_flat, units=layer1_unit_count, activation=tf.nn.relu)\n",
    "dropout1 = tf.layers.dropout(inputs=dense1, rate=dropout_rate)\n",
    "\n",
    "dense2 = tf.layers.dense(inputs=dropout1, units=layer1_unit_count, activation=tf.nn.relu)\n",
    "dropout2 = tf.layers.dropout(inputs=dense2, rate=dropout_rate)\n",
    "\n",
    "dense3 = tf.layers.dense(inputs=dropout2, units=layer1_unit_count, activation=tf.nn.relu)\n",
    "dropout3 = tf.layers.dropout(inputs=dense3, rate=dropout_rate)\n",
    "\n",
    "dense4 = tf.layers.dense(inputs=dropout3, units=layer1_unit_count, activation=tf.nn.relu)\n",
    "dropout4 = tf.layers.dropout(inputs=dense4, rate=dropout_rate)\n",
    "\n",
    "netout = tf.layers.dense(inputs=dropout4, units=18)\n",
    "softout = tf.nn.softmax(netout, name=\"out\")\n",
    "\n",
    "# --- loss 함수 정의 ---\n",
    "# 평균 제곱 오차 함수 사용\n",
    "y = tf.placeholder(tf.float32, [None, 18])\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(softout - y), 1))\n",
    "loss_summ = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# --- loss 함수의 최적화 함수 정의 ---\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 0   loss= 0.9444451\n",
      "index= 100   loss= 0.37848797\n",
      "index= 200   loss= 0.12172048\n",
      "index= 300   loss= 0.07446921\n",
      "index= 400   loss= 0.06009061\n",
      "index= 500   loss= 0.06715868\n",
      "index= 600   loss= 0.0037801175\n",
      "index= 700   loss= 0.0008391685\n",
      "index= 800   loss= 0.0005388473\n",
      "index= 900   loss= 0.00045083745\n",
      "index= 1000   loss= 0.00033395382\n",
      "index= 1100   loss= 0.00029730875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1227 14:42:33.562742 140558034323264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 1199   loss= 9.805294e-05\n",
      "running time: 516.5799572467804\n",
      "save model\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# --- 학습 수행 및 모델 저장---\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # tensorboard 용 로깅 정의    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tblogdir)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    # 신경망 가중치(weight) 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 전체 데이터에 대해 5000번 수행\n",
    "    # loss가 0.001 이하일때, 학습 중단\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_count):\n",
    "        \n",
    "        # 학습용 데이터와 라벨\n",
    "        # 랜덤 데이터를 training_count 만큼 가져옴\n",
    "        base_index = np.random.randint(noise_label_list.shape[0] - training_count)\n",
    "        train_x = noise_list[base_index:base_index + training_count]\n",
    "        train_y = noise_label_list[base_index:base_index + training_count]\n",
    "\n",
    "        # 학습 수행\n",
    "        _train, _loss_summ, _loss = sess.run([train, loss_summ, loss], feed_dict={x:train_x, y:train_y, dropout_rate:0.4})\n",
    "        writer.add_summary(summary = _loss_summ, global_step = epoch)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"index=\", epoch, \"  loss=\", _loss)\n",
    "            \n",
    "        if _loss < loss_limit:\n",
    "            print(\"index=\", epoch, \"  loss=\", _loss)\n",
    "            break\n",
    "            \n",
    "    end_time = time.time()\n",
    "    print(\"running time:\", end_time - start_time)\n",
    "    \n",
    "    # --- 학습 모델 저장 ----\n",
    "    print(\"save model\")\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(modeldir)\n",
    "    signature = tf.saved_model.predict_signature_def(inputs={\"in\":x}, outputs={\"out\":softout})\n",
    "    builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING], signature_def_map={\"serving_default\":signature})\n",
    "    builder.save()\n",
    "    print(\"complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
